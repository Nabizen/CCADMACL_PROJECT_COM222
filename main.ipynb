{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from stopwordsiso import stopwords as stopwords_iso\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "# from langdetect import detect\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect, LangDetectException\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 5.73MB/s]                    \n",
      "2025-02-22 18:54:22 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-22 18:54:22 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-02-22 18:54:23 INFO: File exists: C:\\Users\\rainn\\stanza_resources\\en\\default.zip\n",
      "2025-02-22 18:54:26 INFO: Finished downloading models and saved to C:\\Users\\rainn\\stanza_resources\n",
      "2025-02-22 18:54:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 15.7MB/s]                    \n",
      "2025-02-22 18:54:26 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-22 18:54:26 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-02-22 18:54:26 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-02-22 18:54:26 WARNING: GPU requested, but is not available!\n",
      "2025-02-22 18:54:26 INFO: Using device: cpu\n",
      "2025-02-22 18:54:26 INFO: Loading: tokenize\n",
      "2025-02-22 18:54:28 INFO: Loading: mwt\n",
      "2025-02-22 18:54:28 INFO: Loading: lemma\n",
      "2025-02-22 18:54:29 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rainn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PROCESSORS\n",
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "stanza.download('en')\n",
    "# TAGALOG TOKENIZER FOR SENTENCES THAT ARE DOMINANT IN TAGALOG WORDS (used to segment sentences into a bag of words)\n",
    "nlp = stanza.Pipeline('en', processors=\"tokenize,lemma\",use_gpu=True, treebank='ESLSpok')\n",
    "# STOP WORDS for both languages\n",
    "nltk.download('stopwords')\n",
    "stopwords_all = stopwords_nltk.words('english') + list(stopwords_iso('tl')) + [\"im\", \"i'm\", \"i am\"]\n",
    "\n",
    "# translator\n",
    "# translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# pipeline for bert transformer\n",
    "# bert_pipe_tiai = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagalog_contractions = {\n",
    "    'sya': 'siya',\n",
    "    'bat': 'bakit',\n",
    "    'ikay': 'ikaw ay',\n",
    "    'sakin': 'sa akin',\n",
    "    'sayo': 'sa iyo',\n",
    "    'samin': 'sa amin',\n",
    "    'withraw':'withdraw',\n",
    "    'satin': 'sa atin',\n",
    "    'niyo':'ninyo',\n",
    "    'nyo': 'ninyo',\n",
    "    'nang': 'na ang',\n",
    "    'tas': 'tapos',\n",
    "    'meron': 'mayroon',\n",
    "    'penge': 'pahingi',\n",
    "    'di' : 'hindi',\n",
    "    'pls': 'please',\n",
    "    'sna': 'sana',\n",
    "    'gomana': 'gumana',\n",
    "    'dili': 'hindi',\n",
    "    'hoi': 'hey',\n",
    "    'dko': 'hindi ko',\n",
    "    'nmn':'',\n",
    "    'naman':'',\n",
    "    'nman':'',\n",
    "    'kaka':'',\n",
    "    'bkt': 'bakit',\n",
    "    'prin': 'parin',\n",
    "    'nkaka':'',\n",
    "    'pla':'pala',\n",
    "    '2fa': 'authentication',\n",
    "    'transac': 'transaction',\n",
    "    'app': 'application',\n",
    "    'nah':'no',\n",
    "    'pesos':'peso',\n",
    "    'php':'peso',\n",
    "    'pera':'money',\n",
    "    'apps':'application',\n",
    "    'bwesit':'bwisit',\n",
    "    'bwiset':'bwisit',\n",
    "    'tngina':'putangina',\n",
    "    'tangina':'putangina',\n",
    "    'tnginaka':'putangina',\n",
    "    'kingina':'putangina',\n",
    "    'mbgal':'bagal',\n",
    "    'mabagal':'bagal',\n",
    "    'gcahs':'gcash',\n",
    "    'gagu':'gago',\n",
    "    'bubu':'bobo',\n",
    "    'ogag':'gago',\n",
    "    'obob':'bobo',\n",
    "    'tgal':'matagal',\n",
    "    'tagal':'matagal',\n",
    "    'mtagal':'matagal',\n",
    "    'accnt':'account',\n",
    "    'recieve':'receive',\n",
    "    'recieved':'receive',\n",
    "    'ive':'have',\n",
    "    'ayosin':'ayusin',\n",
    "    'q': 'ko',\n",
    "    'xa':'siya',\n",
    "    'ug':'iyon',\n",
    "    'yung':'iyon',\n",
    "    'yong':'iyon',\n",
    "    'imong':'iyon',\n",
    "    'hindot':'putangina',\n",
    "    'lage':'palagi',\n",
    "    'laging':'palagi',\n",
    "    'lagi':'palagi',\n",
    "    'palaging':'palagi',\n",
    "    'bkit':'bakit',\n",
    "    'lyk':'like',\n",
    "    'dis':'this',\n",
    "    'nko':'nako',\n",
    "    'de':'hindi',\n",
    "    'dl':'download',\n",
    "    'cp':'phone',\n",
    "    'cellphone':'phone',\n",
    "    'ai':'artificial intelligence',\n",
    "    'nd':'',\n",
    "    'ndi':'hindi',\n",
    "    'hndi':'hindi',\n",
    "    'andami':'mada',\n",
    "    'pinag':'',\n",
    "    'kapag':'',\n",
    "    'naka':'',\n",
    "    'nakaka':'',\n",
    "    'nkka':'',\n",
    "    'dabest':'best', \n",
    "    'pakyu':'putangina',\n",
    "    'ampanget':'pangit',\n",
    "    'panget':'pangit',\n",
    "    'hina':'mahina',\n",
    "    \n",
    "}\n",
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"without\",\n",
    "    \"don't\", \"do not\", \"doesn't\", \"does not\", \"didn't\", \"did not\",\n",
    "    \"can't\", \"cannot\", \"couldn't\", \"could not\", \"won't\", \"will not\",\n",
    "    \"wouldn't\", \"would not\", \"shouldn't\", \"should not\",\n",
    "    \"isn't\", \"is not\", \"aren't\", \"are not\", \"wasn't\", \"was not\", \"weren't\", \"were not\",\n",
    "    \"hasn't\", \"has not\", \"haven't\", \"have not\", \"hadn't\", \"had not\",\n",
    "\n",
    "    \"hindi\", \"hnd\", \"ndi\", \"wala\", \"walang\",\n",
    "    \"ayaw\", \"wag\", \"huwag\", \"hnd pa\", \"ndi pa\", \"di pa\", \"wala pa\",\n",
    "    \"huwag na\", \"hnd pa\", \"hindi pa\", \"ayoko\", \"di ako\", \"wala nang\",\n",
    "    \"di mo\", \"d mo\", \"di siya\", \"d siya\", \"di sila\",\n",
    "    \"hindi rin\", \"hnd rin\", \"hindi naman\", \"wala ito\", \"wala akong\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "    # stopwords_nltk.words('english')\n",
    "# print(bert_pipe_tiai(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"this is not good gchash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DF ONLY GET IMPORTANT COLUMNS\n",
    "load_list = ['2020-','2021-','2022']\n",
    "def load_df(load):\n",
    "    # removes words that are less than 3 letters, no letters, stopwords, contractions, unnecessary columns, and null values\n",
    "    data = pd.read_csv('GCASH_REVIEWS.csv')\n",
    "    data = data[data['review_text'].str.contains(r'[a-zA-Z]', na=False)]\n",
    "    data = data[data['review_text'].str.len() > 3]\n",
    "    data = data[~data['review_text'].isin(tagalog_contractions)]\n",
    "    data = data[~data['review_text'].isin(stopwords_all)]\n",
    "    data = data.drop(columns=['author_app_version', 'author_id', 'author_name'])\n",
    "    data = data.dropna()\n",
    "    original = data.copy()\n",
    "    # FILTER BY YEAR just change 20XX-\n",
    "    data = data[data['review_datetime_utc'].str.startswith(load)]\n",
    "    return data, original\n",
    "\n",
    "# df, df_original = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'text' is one row of string of words for every row in df['review_text']\n",
    "def handleNegation(text: str) -> str:\n",
    "    for neg in negation_words:\n",
    "        text = text.replace(\" \"+neg+\" \", \" negativelabel \")\n",
    "    return text\n",
    "\n",
    "label_obj = {\"Very Negative\" : 'verynegativelabel', \"Negative\": 'negativelabel', \"Neutral\": 'neutrallabel', \"Positive\": 'positivelabel', \"Very Positive\": 'verypositivelabel'}\n",
    "def clean_text(text: str):\n",
    "    \n",
    "    # handles negation identification and adds negative label as placeholder to not be removed in stopwords\n",
    "    text = text.lower()\n",
    "    text_negation_handled = handleNegation(text)\n",
    "    \n",
    "    # filters text, remove special characters but keeps hyphened words\n",
    "    # text = re.sub(r\"\\b(nkka|nakaka|nka|naka|nagka|nagkaka|magka|magkaka|mgka|mgkaka|)\\w*\\b\", \"\", text_negation_handled)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_negation_handled)\n",
    "    text = text.replace(\"g cash\", \"gcash\")\n",
    "    \n",
    "    # turn tagalogs into their non contraction form if available (e.g satin -> sa atin)\n",
    "    words = text.split()\n",
    "    non_contracted_words = [(tagalog_contractions[w] if w in tagalog_contractions else w) for w in words]\n",
    "\n",
    "    # exclude stopwords for english and tagalog (e.g ang, may, ako, myself, is, the)\n",
    "    no_stop_words = [ncw for ncw in non_contracted_words if ncw not in stopwords_all]\n",
    "\n",
    "    # do lemmatizing to identify words of context\n",
    "    filtered_words = ' '.join(no_stop_words)\n",
    "    filtered_words = filtered_words.lower()\n",
    "\n",
    "    doc = nlp(filtered_words)\n",
    "    \n",
    "    principal_words = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            w = word.lemma if word.lemma else word.text\n",
    "            # print(w, detect(w))\n",
    "            if len(w) < 2 or w.isdigit(): \n",
    "                continue\n",
    "            # if detect(w) in ['en', 'tl']: \n",
    "            principal_words.append(w)\n",
    "    \n",
    "    # printouts\n",
    "    # print(\"negation handled: \", text_negation_handled)\n",
    "    # print(\"regex applied: \", text)\n",
    "    # print(\"noncontractedwords \", non_contracted_words)\n",
    "    # print(\"no stop words\", no_stop_words)\n",
    "    # print(\"filtered_words \", filtered_words)\n",
    "    # print(principal_words)\n",
    "    # then return an array of words\n",
    "    return principal_words if len(principal_words) > 2 else None\n",
    "            \n",
    "# clean_text(\"Ok ðŸŽ‰ sya gamitin ung SSS .but sometimes may difficulties,...but worthit ..na intact parin yung laman ng gcash? ðŸ˜‚\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for load in load_list:\n",
    "    df, _ = load_df(load)\n",
    "    df['review_text_formatted'] = [clean_text(text) for text in tqdm(df['review_text'])]\n",
    "    df.to_csv('gcash-{load}formatted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df.sample(100)\n",
    "# df_sample['review_text_formatted'] = [clean_text(text) for text in tqdm(df_sample['review_text'])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
