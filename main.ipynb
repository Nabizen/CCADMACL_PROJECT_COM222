{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from stopwordsiso import stopwords as stopwords_iso\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "# from langdetect import detect\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 29.8MB/s]                    \n",
      "2025-02-15 13:59:05 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-15 13:59:05 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-02-15 13:59:06 INFO: File exists: C:\\Users\\rainn\\stanza_resources\\en\\default.zip\n",
      "2025-02-15 13:59:09 INFO: Finished downloading models and saved to C:\\Users\\rainn\\stanza_resources\n",
      "2025-02-15 13:59:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 61.5MB/s]                    \n",
      "2025-02-15 13:59:09 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-15 13:59:09 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-02-15 13:59:09 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-02-15 13:59:09 WARNING: GPU requested, but is not available!\n",
      "2025-02-15 13:59:09 INFO: Using device: cpu\n",
      "2025-02-15 13:59:09 INFO: Loading: tokenize\n",
      "2025-02-15 13:59:09 INFO: Loading: mwt\n",
      "2025-02-15 13:59:09 INFO: Loading: lemma\n",
      "2025-02-15 13:59:10 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rainn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PROCESSORS\n",
    "stanza.download('en')\n",
    "# TAGALOG TOKENIZER FOR SENTENCES THAT ARE DOMINANT IN TAGALOG WORDS (used to segment sentences into a bag of words)\n",
    "nlp = stanza.Pipeline('en', processors=\"tokenize,lemma\",use_gpu=True, treebank='ESLSpok')\n",
    "# STOP WORDS for both languages\n",
    "nltk.download('stopwords')\n",
    "stopwords_all = stopwords_nltk.words('english') + list(stopwords_iso('tl')) + [\"im\", \"i'm\", \"i am\"]\n",
    "\n",
    "# translator\n",
    "# translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# pipeline for bert transformer\n",
    "# bert_pipe_tiai = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagalog_contractions = {\n",
    "    'sya': 'siya',\n",
    "    'bat': 'bakit',\n",
    "    'ikay': 'ikaw ay',\n",
    "    'sakin': 'sa akin',\n",
    "    'sayo': 'sa iyo',\n",
    "    'samin': 'sa amin',\n",
    "    'satin': 'sa atin',\n",
    "    'nyo': 'ninyo',\n",
    "    'nang': 'na ang',\n",
    "    'tas': 'tapos',\n",
    "    'meron': 'mayroon',\n",
    "    'penge': 'pahingi',\n",
    "    'di' : 'hindi',\n",
    "    'pls': 'please',\n",
    "    'sna': 'sana',\n",
    "    'gomana': 'gumana',\n",
    "    'dili': 'hindi',\n",
    "    'hoi': 'hey',\n",
    "    'dko': 'hindi ko',\n",
    "    'nmn':'naman',\n",
    "    'bkt': 'bakit',\n",
    "    'prin': 'parin',\n",
    "    'nkaka':'nakaka',\n",
    "    'pla':'pala',\n",
    "    '2fa': 'two factor authentication',\n",
    "    'transac': 'transaction',\n",
    "    'app': 'application',\n",
    "    'nah':'no',\n",
    "    'pesos':'peso',\n",
    "    'php':'peso',\n",
    "    'pera':'money',\n",
    "    'apps':'application',\n",
    "    'bwesit':'bwisit',\n",
    "    'bwiset':'bwisit',\n",
    "    'tngina':'putangina',\n",
    "    'tangina':'putangina',\n",
    "    'tnginaka':'putangina',\n",
    "    'kingina':'putangina',\n",
    "    'mbgal':'bagal',\n",
    "    'mabagal':'bagal',\n",
    "    'gcahs':'gcash',\n",
    "    'gagu':'gago',\n",
    "    'bubu':'bobo',\n",
    "    'ogag':'gago',\n",
    "    'obob':'bobo',\n",
    "    'tgal':'tagal',\n",
    "    'accnt':'account',\n",
    "    'recieve':'receive',\n",
    "    'recieved':'receive',\n",
    "    'ive':'have',\n",
    "    'ayosin':'ayusin',\n",
    "    'q': 'ko',\n",
    "    'xa':'siya',\n",
    "    'ug':'iyon',\n",
    "    'yung':'iyon',\n",
    "    'yong':'iyon',\n",
    "    'imong':'iyon',\n",
    "    'hindot':'putangina',\n",
    "    'laging':'palagi',\n",
    "    'lagi':'palagi',\n",
    "    'palaging':'palagi',\n",
    "    'bkit':'bakit',\n",
    "    'lyk':'like',\n",
    "    'dis':'this',\n",
    "    'nko':'nako',\n",
    "    'de':'hindi',\n",
    "    'dl':'download',\n",
    "    'cp':'phone',\n",
    "    'cellphone':'phone',\n",
    "    'ai':'artificial intelligence',\n",
    "    'nd':'',\n",
    "    'ndi':'hindi',\n",
    "}\n",
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"without\",\n",
    "    \"don't\", \"do not\", \"doesn't\", \"does not\", \"didn't\", \"did not\",\n",
    "    \"can't\", \"cannot\", \"couldn't\", \"could not\", \"won't\", \"will not\",\n",
    "    \"wouldn't\", \"would not\", \"shouldn't\", \"should not\",\n",
    "    \"isn't\", \"is not\", \"aren't\", \"are not\", \"wasn't\", \"was not\", \"weren't\", \"were not\",\n",
    "    \"hasn't\", \"has not\", \"haven't\", \"have not\", \"hadn't\", \"had not\",\n",
    "\n",
    "    \"hindi\", \"hnd\", \"ndi\", \"wala\", \"walang\",\n",
    "    \"ayaw\", \"wag\", \"huwag\", \"hnd pa\", \"ndi pa\", \"di pa\", \"wala pa\",\n",
    "    \"huwag na\", \"hnd pa\", \"hindi pa\", \"ayoko\", \"di ako\", \"wala nang\",\n",
    "    \"di mo\", \"d mo\", \"di siya\", \"d siya\", \"di sila\",\n",
    "    \"hindi rin\", \"hnd rin\", \"hindi naman\", \"wala ito\", \"wala akong\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "    # stopwords_nltk.words('english')\n",
    "# print(bert_pipe_tiai(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"this is not good gchash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DF ONLY GET IMPORTANT COLUMNS\n",
    "def load_df():\n",
    "    # removes words that are less than 3 letters, no letters, stopwords, contractions, unnecessary columns, and null values\n",
    "    data = pd.read_csv('GCASH_REVIEWS.csv')\n",
    "    data = data[data['review_text'].str.contains(r'[a-zA-Z]', na=False)]\n",
    "    data = data[data['review_text'].str.len() > 3]\n",
    "    data = data[~data['review_text'].isin(tagalog_contractions)]\n",
    "    data = data[~data['review_text'].isin(stopwords_all)]\n",
    "    data = data.drop(columns=['author_app_version', 'author_id', 'author_name'])\n",
    "    data = data.dropna()\n",
    "    original = data.copy()\n",
    "    # FILTER BY YEAR just change 20XX-\n",
    "    data = data[data['review_datetime_utc'].str.startswith('2023-')]\n",
    "    return data, original\n",
    "\n",
    "df, df_original = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'text' is one row of string of words for every row in df['review_text']\n",
    "def handleNegation(text: str) -> str:\n",
    "    for neg in negation_words:\n",
    "        text = text.replace(\" \"+neg+\" \", \" negativelabel \")\n",
    "    return text\n",
    "\n",
    "label_obj = {\"Very Negative\" : 'verynegativelabel', \"Negative\": 'negativelabel', \"Neutral\": 'neutrallabel', \"Positive\": 'positivelabel', \"Very Positive\": 'verypositivelabel'}\n",
    "def clean_text(text: str):\n",
    "    \n",
    "    # handles negation identification and adds negative label as placeholder to not be removed in stopwords\n",
    "    text = text.lower()\n",
    "    text_negation_handled = handleNegation(text)\n",
    "    \n",
    "    # filters text, remove special characters but keeps hyphened words\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_negation_handled)\n",
    "    text = text.replace(\"g cash\", \"gcash\")\n",
    "    \n",
    "    # turn tagalogs into their non contraction form if available (e.g satin -> sa atin)\n",
    "    words = text.split()\n",
    "    non_contracted_words = [(tagalog_contractions[w] if w in tagalog_contractions else w) for w in words]\n",
    "\n",
    "    # exclude stopwords for english and tagalog (e.g ang, may, ako, myself, is, the)\n",
    "    no_stop_words = [ncw for ncw in non_contracted_words if ncw not in stopwords_all]\n",
    "\n",
    "    # do lemmatizing to identify words of context\n",
    "    filtered_words = ' '.join(no_stop_words)\n",
    "    filtered_words = filtered_words.lower()\n",
    "\n",
    "    doc = nlp(filtered_words)\n",
    "    \n",
    "    principal_words = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            principal_words.append(word.lemma if word.lemma else word.text)\n",
    "    \n",
    "    # printouts\n",
    "    # print(\"negation handled: \", text_negation_handled)\n",
    "    # print(\"regex applied: \", text)\n",
    "    # print(\"noncontractedwords \", non_contracted_words)\n",
    "    # print(\"no stop words\", no_stop_words)\n",
    "    # print(\"filtered_words \", filtered_words)\n",
    "    \n",
    "    # then return an array of words\n",
    "    return principal_words\n",
    "            \n",
    "# clean_text(\"Ok ðŸŽ‰ sya gamitin ung SSS .but sometimes may difficulties,...but worthit ..na intact parin yung laman ng gcash? ðŸ˜‚\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_text(\"I couldn't get access to the internet even though i have wifi?\")\n",
    "# clean_text(\"I couldn't get lacks wala nang  anuman help from this app even though I'm fully verified.\")\n",
    "clean_text(\"when im in philippines i recieve already money if somebody send me..but yesterday somebody send to my gcash account until now im not recieve the money..what will i do.?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133055/133055 [27:34<00:00, 80.43it/s] \n"
     ]
    }
   ],
   "source": [
    "df['review_text_formatted'] = [clean_text(text) for text in tqdm(df['review_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(100)\n",
    "df_sample['review_text_formatted'] = [clean_text(text) for text in tqdm(df_sample['review_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation handled:   negativelabel nako nmn\n",
      "regex applied:   negativelabel nako nmn\n",
      "noncontractedwords  ['negativelabel', 'nako', 'nmn']\n",
      "no stop words ['negativelabel', 'nako', 'nmn']\n",
      "filtered_words  negativelabel nako nmn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negativelabel', 'nako', 'nmn']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('walanako nmn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('gcash-2023-formatted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
