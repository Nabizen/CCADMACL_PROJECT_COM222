{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from stopwordsiso import stopwords as stopwords_iso\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "# from langdetect import detect\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect, LangDetectException\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 48.2MB/s]                    \n",
      "2025-02-22 18:06:13 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-22 18:06:13 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-02-22 18:06:14 INFO: File exists: C:\\Users\\rainn\\stanza_resources\\en\\default.zip\n",
      "2025-02-22 18:06:18 INFO: Finished downloading models and saved to C:\\Users\\rainn\\stanza_resources\n",
      "2025-02-22 18:06:18 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 47.0MB/s]                    \n",
      "2025-02-22 18:06:18 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-22 18:06:18 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-02-22 18:06:18 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-02-22 18:06:18 WARNING: GPU requested, but is not available!\n",
      "2025-02-22 18:06:18 INFO: Using device: cpu\n",
      "2025-02-22 18:06:18 INFO: Loading: tokenize\n",
      "2025-02-22 18:06:18 INFO: Loading: mwt\n",
      "2025-02-22 18:06:18 INFO: Loading: lemma\n",
      "2025-02-22 18:06:19 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rainn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PROCESSORS\n",
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "stanza.download('en')\n",
    "# TAGALOG TOKENIZER FOR SENTENCES THAT ARE DOMINANT IN TAGALOG WORDS (used to segment sentences into a bag of words)\n",
    "nlp = stanza.Pipeline('en', processors=\"tokenize,lemma\",use_gpu=True, treebank='ESLSpok')\n",
    "# STOP WORDS for both languages\n",
    "nltk.download('stopwords')\n",
    "stopwords_all = stopwords_nltk.words('english') + list(stopwords_iso('tl')) + [\"im\", \"i'm\", \"i am\"]\n",
    "\n",
    "# translator\n",
    "# translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# pipeline for bert transformer\n",
    "# bert_pipe_tiai = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language.ENGLISH\n"
     ]
    }
   ],
   "source": [
    "confidence_value = detector.detect_language_of(\"aaaswdsxsxw\")\n",
    "print(confidence_value)\n",
    "# print(confidence_value.language.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagalog_contractions = {\n",
    "    'sya': 'siya',\n",
    "    'bat': 'bakit',\n",
    "    'ikay': 'ikaw ay',\n",
    "    'sakin': 'sa akin',\n",
    "    'sayo': 'sa iyo',\n",
    "    'samin': 'sa amin',\n",
    "    'withraw':'withdraw',\n",
    "    'satin': 'sa atin',\n",
    "    'niyo':'ninyo',\n",
    "    'nyo': 'ninyo',\n",
    "    'nang': 'na ang',\n",
    "    'tas': 'tapos',\n",
    "    'meron': 'mayroon',\n",
    "    'penge': 'pahingi',\n",
    "    'di' : 'hindi',\n",
    "    'pls': 'please',\n",
    "    'sna': 'sana',\n",
    "    'gomana': 'gumana',\n",
    "    'dili': 'hindi',\n",
    "    'hoi': 'hey',\n",
    "    'dko': 'hindi ko',\n",
    "    'nmn':'',\n",
    "    'naman':'',\n",
    "    'nman':'',\n",
    "    'kaka':'',\n",
    "    'bkt': 'bakit',\n",
    "    'prin': 'parin',\n",
    "    'nkaka':'',\n",
    "    'pla':'pala',\n",
    "    '2fa': 'authentication',\n",
    "    'transac': 'transaction',\n",
    "    'app': 'application',\n",
    "    'nah':'no',\n",
    "    'pesos':'peso',\n",
    "    'php':'peso',\n",
    "    'pera':'money',\n",
    "    'apps':'application',\n",
    "    'bwesit':'bwisit',\n",
    "    'bwiset':'bwisit',\n",
    "    'tngina':'putangina',\n",
    "    'tangina':'putangina',\n",
    "    'tnginaka':'putangina',\n",
    "    'kingina':'putangina',\n",
    "    'mbgal':'bagal',\n",
    "    'mabagal':'bagal',\n",
    "    'gcahs':'gcash',\n",
    "    'gagu':'gago',\n",
    "    'bubu':'bobo',\n",
    "    'ogag':'gago',\n",
    "    'obob':'bobo',\n",
    "    'tgal':'matagal',\n",
    "    'tagal':'matagal',\n",
    "    'mtagal':'matagal',\n",
    "    'accnt':'account',\n",
    "    'recieve':'receive',\n",
    "    'recieved':'receive',\n",
    "    'ive':'have',\n",
    "    'ayosin':'ayusin',\n",
    "    'q': 'ko',\n",
    "    'xa':'siya',\n",
    "    'ug':'iyon',\n",
    "    'yung':'iyon',\n",
    "    'yong':'iyon',\n",
    "    'imong':'iyon',\n",
    "    'hindot':'putangina',\n",
    "    'lage':'palagi',\n",
    "    'laging':'palagi',\n",
    "    'lagi':'palagi',\n",
    "    'palaging':'palagi',\n",
    "    'bkit':'bakit',\n",
    "    'lyk':'like',\n",
    "    'dis':'this',\n",
    "    'nko':'nako',\n",
    "    'de':'hindi',\n",
    "    'dl':'download',\n",
    "    'cp':'phone',\n",
    "    'cellphone':'phone',\n",
    "    'ai':'artificial intelligence',\n",
    "    'nd':'',\n",
    "    'ndi':'hindi',\n",
    "    'hndi':'hindi',\n",
    "    'andami':'mada',\n",
    "    'pinag':'',\n",
    "    'kapag':'',\n",
    "    'naka':'',\n",
    "    'nakaka':'',\n",
    "    'nkka':'',\n",
    "    'dabest':'best', \n",
    "    'pakyu':'putangina',\n",
    "    'ampanget':'pangit',\n",
    "    'panget':'pangit',\n",
    "    'hina':'mahina',\n",
    "    \n",
    "}\n",
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"without\",\n",
    "    \"don't\", \"do not\", \"doesn't\", \"does not\", \"didn't\", \"did not\",\n",
    "    \"can't\", \"cannot\", \"couldn't\", \"could not\", \"won't\", \"will not\",\n",
    "    \"wouldn't\", \"would not\", \"shouldn't\", \"should not\",\n",
    "    \"isn't\", \"is not\", \"aren't\", \"are not\", \"wasn't\", \"was not\", \"weren't\", \"were not\",\n",
    "    \"hasn't\", \"has not\", \"haven't\", \"have not\", \"hadn't\", \"had not\",\n",
    "\n",
    "    \"hindi\", \"hnd\", \"ndi\", \"wala\", \"walang\",\n",
    "    \"ayaw\", \"wag\", \"huwag\", \"hnd pa\", \"ndi pa\", \"di pa\", \"wala pa\",\n",
    "    \"huwag na\", \"hnd pa\", \"hindi pa\", \"ayoko\", \"di ako\", \"wala nang\",\n",
    "    \"di mo\", \"d mo\", \"di siya\", \"d siya\", \"di sila\",\n",
    "    \"hindi rin\", \"hnd rin\", \"hindi naman\", \"wala ito\", \"wala akong\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "    # stopwords_nltk.words('english')\n",
    "# print(bert_pipe_tiai(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"this is not good gchash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DF ONLY GET IMPORTANT COLUMNS\n",
    "def load_df():\n",
    "    # removes words that are less than 3 letters, no letters, stopwords, contractions, unnecessary columns, and null values\n",
    "    data = pd.read_csv('GCASH_REVIEWS.csv')\n",
    "    data = data[data['review_text'].str.contains(r'[a-zA-Z]', na=False)]\n",
    "    data = data[data['review_text'].str.len() > 3]\n",
    "    data = data[~data['review_text'].isin(tagalog_contractions)]\n",
    "    data = data[~data['review_text'].isin(stopwords_all)]\n",
    "    data = data.drop(columns=['author_app_version', 'author_id', 'author_name'])\n",
    "    data = data.dropna()\n",
    "    original = data.copy()\n",
    "    # FILTER BY YEAR just change 20XX-\n",
    "    data = data[data['review_datetime_utc'].str.startswith('2023-')]\n",
    "    return data, original\n",
    "\n",
    "df, df_original = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'text' is one row of string of words for every row in df['review_text']\n",
    "def handleNegation(text: str) -> str:\n",
    "    for neg in negation_words:\n",
    "        text = text.replace(\" \"+neg+\" \", \" negativelabel \")\n",
    "    return text\n",
    "\n",
    "label_obj = {\"Very Negative\" : 'verynegativelabel', \"Negative\": 'negativelabel', \"Neutral\": 'neutrallabel', \"Positive\": 'positivelabel', \"Very Positive\": 'verypositivelabel'}\n",
    "def clean_text(text: str):\n",
    "    \n",
    "    # handles negation identification and adds negative label as placeholder to not be removed in stopwords\n",
    "    text = text.lower()\n",
    "    text_negation_handled = handleNegation(text)\n",
    "    \n",
    "    # filters text, remove special characters but keeps hyphened words\n",
    "    # text = re.sub(r\"\\b(nkka|nakaka|nka|naka|nagka|nagkaka|magka|magkaka|mgka|mgkaka|)\\w*\\b\", \"\", text_negation_handled)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_negation_handled)\n",
    "    text = text.replace(\"g cash\", \"gcash\")\n",
    "    \n",
    "    # turn tagalogs into their non contraction form if available (e.g satin -> sa atin)\n",
    "    words = text.split()\n",
    "    non_contracted_words = [(tagalog_contractions[w] if w in tagalog_contractions else w) for w in words]\n",
    "\n",
    "    # exclude stopwords for english and tagalog (e.g ang, may, ako, myself, is, the)\n",
    "    no_stop_words = [ncw for ncw in non_contracted_words if ncw not in stopwords_all]\n",
    "\n",
    "    # do lemmatizing to identify words of context\n",
    "    filtered_words = ' '.join(no_stop_words)\n",
    "    filtered_words = filtered_words.lower()\n",
    "\n",
    "    doc = nlp(filtered_words)\n",
    "    \n",
    "    principal_words = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            w = word.lemma if word.lemma else word.text\n",
    "            # print(w, detect(w))\n",
    "            if len(w) < 2 or w.isdigit(): \n",
    "                continue\n",
    "            # if detect(w) in ['en', 'tl']: \n",
    "            principal_words.append(w)\n",
    "    \n",
    "    # printouts\n",
    "    # print(\"negation handled: \", text_negation_handled)\n",
    "    # print(\"regex applied: \", text)\n",
    "    # print(\"noncontractedwords \", non_contracted_words)\n",
    "    # print(\"no stop words\", no_stop_words)\n",
    "    # print(\"filtered_words \", filtered_words)\n",
    "    # print(principal_words)\n",
    "    # then return an array of words\n",
    "    return principal_words if len(principal_words) > 2 else None\n",
    "            \n",
    "# clean_text(\"Ok ðŸŽ‰ sya gamitin ung SSS .but sometimes may difficulties,...but worthit ..na intact parin yung laman ng gcash? ðŸ˜‚\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'da'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"send\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['philippine',\n",
       " 'receive',\n",
       " 'already',\n",
       " 'money',\n",
       " 'somebody',\n",
       " 'send',\n",
       " 'yesterday',\n",
       " 'somebody',\n",
       " 'send',\n",
       " 'gcash',\n",
       " 'account',\n",
       " 'negativelabel',\n",
       " 'receive',\n",
       " 'money']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean_text(\"I couldn't get access to the internet even though i have wifi?\")\n",
    "# clean_text(\"I couldn't get lacks wala nang  anuman help from this app even though I'm fully verified.\")\n",
    "clean_text(\"when im in philippines i recieve already money if somebody send me..but yesterday somebody send to my gcash account until now im not recieve the money..what will i do.?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1016/133050 [00:16<35:10, 62.56it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text_formatted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "Cell \u001b[1;32mIn[57], line 30\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     27\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(no_stop_words)\n\u001b[0;32m     28\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m filtered_words\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m---> 30\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m principal_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences:\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\pipeline\\core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\pipeline\\core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[0;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[1;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\pipeline\\lemma_processor.py:94\u001b[0m, in \u001b[0;36mLemmaProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     92\u001b[0m edits \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq2seq_batch):\n\u001b[1;32m---> 94\u001b[0m     ps, es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq2seq_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ps\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m es \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:114\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, batch, beam_size, vocab)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    113\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m preds, edit_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m preds] \u001b[38;5;66;03m# unmap to tokens\u001b[39;00m\n\u001b[0;32m    116\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprune_decoded_seqs(pred_seqs)\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\models\\common\\seq2seq_model.py:301\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[1;34m(self, src, src_mask, pos, beam_size, raw, never_decode_unk)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Predict with beam search. \"\"\"\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beam_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_decode_unk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnever_decode_unk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m enc_inputs, batch_size, src_lens, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(src, src_mask, pos, raw)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# (1) encode source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\models\\common\\seq2seq_model.py:259\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict_greedy\u001b[1;34m(self, src, src_mask, pos, raw, never_decode_unk)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_greedy\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, never_decode_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Predict with greedy decoding. \"\"\"\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m     enc_inputs, batch_size, src_lens, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# encode source\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     h_in, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(enc_inputs, src_lens)\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\stanza\\models\\common\\seq2seq_model.py:230\u001b[0m, in \u001b[0;36mSeq2SeqModel.embed\u001b[1;34m(self, src, src_mask, pos, raw)\u001b[0m\n\u001b[0;32m    228\u001b[0m         raw_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([raw_inputs, raw_zeros], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    229\u001b[0m     enc_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([enc_inputs, raw_inputs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m src_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPAD_ID\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc_inputs, batch_size, src_lens, src_mask\n",
      "File \u001b[1;32mc:\\Users\\rainn\\miniconda3\\envs\\mainenv\\Lib\\site-packages\\torch\\_tensor.py:1164\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[0;32m   1156\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1162\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1163\u001b[0m     )\n\u001b[1;32m-> 1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['review_text_formatted'] = [clean_text(text) for text in tqdm(df['review_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 86.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.sample(100)\n",
    "df_sample['review_text_formatted'] = [clean_text(text) for text in tqdm(df_sample['review_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation handled:   negativelabel nako nmn\n",
      "regex applied:   negativelabel nako nmn\n",
      "noncontractedwords  ['negativelabel', 'nako', 'nmn']\n",
      "no stop words ['negativelabel', 'nako', 'nmn']\n",
      "filtered_words  negativelabel nako nmn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negativelabel', 'nako', 'nmn']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text('walanako nmn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('gcash-2023-formatted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
