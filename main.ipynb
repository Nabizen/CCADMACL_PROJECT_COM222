{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import re\n",
    "from stopwordsiso import stopwords as stopwords_iso\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "# from langdetect import detect\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect, LangDetectException\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 36.9MB/s]                    \n",
      "2025-02-24 13:43:30 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-24 13:43:30 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-02-24 13:43:31 INFO: File exists: C:\\Users\\rainn\\stanza_resources\\en\\default.zip\n",
      "2025-02-24 13:43:34 INFO: Finished downloading models and saved to C:\\Users\\rainn\\stanza_resources\n",
      "2025-02-24 13:43:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 36.2MB/s]                    \n",
      "2025-02-24 13:43:34 INFO: Downloaded file to C:\\Users\\rainn\\stanza_resources\\resources.json\n",
      "2025-02-24 13:43:34 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-02-24 13:43:34 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-02-24 13:43:34 WARNING: GPU requested, but is not available!\n",
      "2025-02-24 13:43:34 INFO: Using device: cpu\n",
      "2025-02-24 13:43:34 INFO: Loading: tokenize\n",
      "2025-02-24 13:43:34 INFO: Loading: mwt\n",
      "2025-02-24 13:43:35 INFO: Loading: lemma\n",
      "2025-02-24 13:43:35 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rainn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PROCESSORS\n",
    "languages = [Language.ENGLISH, Language.TAGALOG]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "stanza.download('en')\n",
    "# TAGALOG TOKENIZER FOR SENTENCES THAT ARE DOMINANT IN TAGALOG WORDS (used to segment sentences into a bag of words)\n",
    "nlp = stanza.Pipeline('en', processors=\"tokenize,lemma\",use_gpu=True, treebank='ESLSpok')\n",
    "# STOP WORDS for both languages\n",
    "nltk.download('stopwords')\n",
    "stopwords_all = stopwords_nltk.words('english') + list(stopwords_iso('tl')) + [\"im\", \"i'm\", \"i am\"]\n",
    "\n",
    "# translator\n",
    "# translator = GoogleTranslator(source='auto', target='en')\n",
    "\n",
    "# pipeline for bert transformer\n",
    "# bert_pipe_tiai = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagalog_contractions = {\n",
    "    'sya': 'siya',\n",
    "    'bat': 'bakit',\n",
    "    'ikay': 'ikaw ay',\n",
    "    'sakin': 'sa akin',\n",
    "    'sayo': 'sa iyo',\n",
    "    'samin': 'sa amin',\n",
    "    'withraw':'withdraw',\n",
    "    'satin': 'sa atin',\n",
    "    'niyo':'ninyo',\n",
    "    'nyo': 'ninyo',\n",
    "    'nang': 'na ang',\n",
    "    'tas': 'tapos',\n",
    "    'meron': 'mayroon',\n",
    "    'penge': 'pahingi',\n",
    "    'di' : 'hindi',\n",
    "    'pls': 'please',\n",
    "    'sna': 'sana',\n",
    "    'gomana': 'gumana',\n",
    "    'dili': 'hindi',\n",
    "    'hoi': 'hey',\n",
    "    'dko': 'hindi ko',\n",
    "    'nmn':'',\n",
    "    'naman':'',\n",
    "    'nman':'',\n",
    "    'kaka':'',\n",
    "    'bkt': 'bakit',\n",
    "    'prin': 'parin',\n",
    "    'nkaka':'',\n",
    "    'pla':'pala',\n",
    "    '2fa': 'authentication',\n",
    "    'transac': 'transaction',\n",
    "    'app': 'application',\n",
    "    'nah':'no',\n",
    "    'pesos':'peso',\n",
    "    'php':'peso',\n",
    "    'pera':'money',\n",
    "    'apps':'application',\n",
    "    'bwesit':'bwisit',\n",
    "    'bwiset':'bwisit',\n",
    "    'tngina':'putangina',\n",
    "    'tangina':'putangina',\n",
    "    'tnginaka':'putangina',\n",
    "    'kingina':'putangina',\n",
    "    'mbgal':'bagal',\n",
    "    'mabagal':'bagal',\n",
    "    'gcahs':'gcash',\n",
    "    'gagu':'gago',\n",
    "    'bubu':'bobo',\n",
    "    'ogag':'gago',\n",
    "    'obob':'bobo',\n",
    "    'tgal':'matagal',\n",
    "    'tagal':'matagal',\n",
    "    'mtagal':'matagal',\n",
    "    'accnt':'account',\n",
    "    'recieve':'receive',\n",
    "    'recieved':'receive',\n",
    "    'ive':'have',\n",
    "    'ayosin':'ayusin',\n",
    "    'q': 'ko',\n",
    "    'xa':'siya',\n",
    "    'ug':'iyon',\n",
    "    'yung':'iyon',\n",
    "    'yong':'iyon',\n",
    "    'imong':'iyon',\n",
    "    'hindot':'putangina',\n",
    "    'lage':'palagi',\n",
    "    'laging':'palagi',\n",
    "    'lagi':'palagi',\n",
    "    'palaging':'palagi',\n",
    "    'bkit':'bakit',\n",
    "    'lyk':'like',\n",
    "    'dis':'this',\n",
    "    'nko':'nako',\n",
    "    'de':'hindi',\n",
    "    'dl':'download',\n",
    "    'cp':'phone',\n",
    "    'cellphone':'phone',\n",
    "    'ai':'artificial intelligence',\n",
    "    'nd':'',\n",
    "    'ndi':'hindi',\n",
    "    'hndi':'hindi',\n",
    "    'andami':'mada',\n",
    "    'pinag':'',\n",
    "    'kapag':'',\n",
    "    'naka':'',\n",
    "    'nakaka':'',\n",
    "    'nkka':'',\n",
    "    'dabest':'best', \n",
    "    'pakyu':'putangina',\n",
    "    'ampanget':'pangit',\n",
    "    'panget':'pangit',\n",
    "    'hina':'mahina',\n",
    "    \n",
    "}\n",
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"without\",\n",
    "    \"don't\", \"do not\", \"doesn't\", \"does not\", \"didn't\", \"did not\",\n",
    "    \"can't\", \"cannot\", \"couldn't\", \"could not\", \"won't\", \"will not\",\n",
    "    \"wouldn't\", \"would not\", \"shouldn't\", \"should not\",\n",
    "    \"isn't\", \"is not\", \"aren't\", \"are not\", \"wasn't\", \"was not\", \"weren't\", \"were not\",\n",
    "    \"hasn't\", \"has not\", \"haven't\", \"have not\", \"hadn't\", \"had not\",\n",
    "    \"cant\", \"couldnt\", \"doesnt\", \"wouldnt\", \"dont\", \"wasnt\", \"werent\", \"hadnt\", \"arent\", \"wont\", \"shouldnt\", \"havent\", \n",
    "    \n",
    "    \"hindi\", \"hnd\", \"ndi\", \"wala\", \"walang\",\n",
    "    \"ayaw\", \"wag\", \"huwag\", \"hnd pa\", \"ndi pa\", \"di pa\", \"wala pa\",\n",
    "    \"huwag na\", \"hnd pa\", \"hindi pa\", \"ayoko\", \"di ako\", \"wala nang\",\n",
    "    \"di mo\", \"d mo\", \"di siya\", \"d siya\", \"di sila\",\n",
    "    \"hindi rin\", \"hnd rin\", \"hindi naman\", \"wala ito\", \"wala akong\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND\n",
    "    # stopwords_nltk.words('english')\n",
    "# print(bert_pipe_tiai(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"Putanginaaaa mong gcash ka andami konang Pera nagloko pa madaya yang putanginang yan Wag nyo to i, install putangggina\"))\n",
    "# print(bert_pipe_dost(\"this is not good gchash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DF ONLY GET IMPORTANT COLUMNS\n",
    "load_list = ['2020-','2021-','2022-', '2023-']\n",
    "def load_df(load):\n",
    "    # removes words that are less than 3 letters, no letters, stopwords, contractions, unnecessary columns, and null values\n",
    "    data = pd.read_csv('GCASH_REVIEWS.csv')\n",
    "    data = data[data['review_text'].str.contains(r'[a-zA-Z]', na=False)]\n",
    "    data = data[data['review_text'].str.len() > 3]\n",
    "    data = data[~data['review_text'].isin(tagalog_contractions)]\n",
    "    data = data[~data['review_text'].isin(stopwords_all)]\n",
    "    data = data.drop(columns=['author_app_version', 'author_id', 'author_name'])\n",
    "    data = data.dropna()\n",
    "    original = data.copy()\n",
    "    # FILTER BY YEAR just change 20XX-\n",
    "    data = data[data['review_datetime_utc'].str.startswith(load)]\n",
    "    return data, original\n",
    "\n",
    "# df, df_original = load_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'text' is one row of string of words for every row in df['review_text']\n",
    "contrast_words = [\"pero\", \"ngunit\", \"subalit\", \"kahit na\", \"pero kasi\", \"but\", \"however\", \"although\", \"though\", \"while\", \"kaso\"]\n",
    "transac_words = {\"log in\": \"login\", \"log out\": \"login\", \"sign in\": \"signin\", \"sign out\": \"signout\", \"g cash\": \"gcash\", \"g-cash\":\"gcash\"}\n",
    "def handleContrast(text: str) -> str:\n",
    "    for contrast in contrast_words:\\\n",
    "        text = re.sub(rf\"\\b{re.escape(contrast)}\\b\", \" contrastlabel \", text)\n",
    "        # text = text.replace(\" \"+contrast+\" \", \" contrastlabel \")\n",
    "    return text\n",
    "\n",
    "def handleNegation(text: str) -> str:\n",
    "    for neg in negation_words:\n",
    "        text = re.sub(rf\"\\b{re.escape(neg)}\\b\", \" negativelabel \", text)\n",
    "        # text = text.replace(\" \"+neg+\" \", \" negativelabel \")\n",
    "    return text\n",
    "def simplifyTransacWords(text: str) -> str:\n",
    "    for tw in transac_words:\n",
    "        text =  re.sub(rf\"\\b{re.escape(tw)}\\b\", \" \"+transac_words[tw]+\" \", text)\n",
    "        # text = text.replace(\" \"+tw+\" \", \" \"+transac_words[tw]+\" \")\n",
    "    return text\n",
    "# label_obj = {\"Very Negative\" : 'verynegativelabel', \"Negative\": 'negativelabel', \"Neutral\": 'neutrallabel', \"Positive\": 'positivelabel', \"Very Positive\": 'verypositivelabel'}\n",
    "def clean_text(text: str):\n",
    "    \n",
    "    # handles negation identification and adds negative label as placeholder to not be removed in stopwords\n",
    "    text = text.lower()\n",
    "    text_negation_handled = handleNegation(text)\n",
    "    \n",
    "    # filters text, remove special characters and emoji\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_negation_handled)\n",
    "    \n",
    "    text_contrast_handled = handleContrast(text)\n",
    "    text = simplifyTransacWords(text_contrast_handled)\n",
    "    \n",
    "    # turn tagalogs into their non contraction (expanded) form if available (e.g satin -> sa atin)\n",
    "    words = text.split()\n",
    "    non_contracted_words = [(tagalog_contractions[w] if w in tagalog_contractions else w) for w in words]\n",
    "\n",
    "    # exclude stopwords for english and tagalog (e.g ang, may, ako, myself, is, the)\n",
    "    no_stop_words = [ncw for ncw in non_contracted_words if ncw not in stopwords_all]\n",
    "\n",
    "    \n",
    "    filtered_words = ' '.join(no_stop_words)\n",
    "    filtered_words = filtered_words.lower()\n",
    "    \n",
    "    # do lemmatizing + base form to identify words of context (e.g running -> run, ran -> run) (english words only)\n",
    "    doc = nlp(filtered_words)\n",
    "    principal_words = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            w = word.lemma if word.lemma else word.text\n",
    "            \n",
    "            # remove standalone numbers, as well gibberish words\n",
    "            if len(w) < 2 or w.isdigit(): \n",
    "                continue\n",
    "            # if detect(w) in ['en', 'tl']: \n",
    "            principal_words.append(w)\n",
    "    \n",
    "    # printouts\n",
    "    \n",
    "    # print(\"regex applied: \", text)\n",
    "    # print(\"negation handled: \", text_negation_handled)\n",
    "    # print(\"contrast handled: \", text_contrast_handled)\n",
    "\n",
    "    # print(\"noncontractedwords \", non_contracted_words)\n",
    "    # print(\"no stop words\", no_stop_words)\n",
    "    # print(\"filtered_words \", filtered_words)\n",
    "    \n",
    "    # then return an array of words, remove low effort/generic reviews \n",
    "    return principal_words if len(principal_words) > 2 else None\n",
    "            \n",
    "# clean_text(\"Ok 🎉 sya gamitin ung SSS .but sometimes may difficulties,...but worthit ..na intact parin yung laman ng gcash? 😂\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' gcash   gcash  sucks'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplifyTransacWords(\"g-cash g cash sucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex applied:   negativelabel  know why  contrastlabel   gcash   gcash  sucks   negativelabel  login or  signin  \n",
      "negation handled:   negativelabel  know why but g-cash g cash sucks,  negativelabel  login or sign-in?\n",
      "contrast handled:   negativelabel  know why  contrastlabel  g cash g cash sucks   negativelabel  login or sign in \n",
      "noncontractedwords  ['negativelabel', 'know', 'why', 'contrastlabel', 'gcash', 'gcash', 'sucks', 'negativelabel', 'login', 'or', 'signin']\n",
      "no stop words ['negativelabel', 'know', 'contrastlabel', 'gcash', 'gcash', 'sucks', 'negativelabel', 'login', 'signin']\n",
      "filtered_words  negativelabel know contrastlabel gcash gcash sucks negativelabel login signin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['negativelabel',\n",
       " 'know',\n",
       " 'contrastlabel',\n",
       " 'gcash',\n",
       " 'gcash',\n",
       " 'suck',\n",
       " 'negativelabel',\n",
       " 'login',\n",
       " 'signin']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"dont know why but g-cash g cash sucks, cant login or sign-in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR:  2020-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72698/72698 [12:45<00:00, 94.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR:  2021-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144343/144343 [24:04<00:00, 99.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR:  2022-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157895/157895 [30:30<00:00, 86.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR:  2023-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133050/133050 [31:16<00:00, 70.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for load in load_list:\n",
    "    print('FOR: ', load)\n",
    "    df, _ = load_df(load)\n",
    "    df['review_text_formatted'] = [clean_text(text) for text in tqdm(df['review_text'])]\n",
    "    df.to_csv('with_contrast/gcash-{}formatted.csv'.format(load))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df.sample(100)\n",
    "# df_sample['review_text_formatted'] = [clean_text(text) for text in tqdm(df_sample['review_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hel;lo2020-\n"
     ]
    }
   ],
   "source": [
    "print('hel;lo{}'.format(load_list[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
